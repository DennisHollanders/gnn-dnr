description: "Debug Config 1 - MLP Switch Head Case"
job_name: "debug_mlp_switch_head"

# Data Configuration
dataset_names: ["train", "validation", "test"]
folder_names: 
  - "data\\split_datasets\\train"
  - "data\\split_datasets\\validation"
  - "data\\split_datasets\\test"

# Model Configuration
model_module: "AdvancedMLP"  
model_kwargs:
  # GAT Configuration (from failed params)
  gnn_type: "GAT"  
  gnn_layers: 6
  gnn_hidden_dim: 96  # Should be divisible by gat_heads (96 % 2 = 0 âœ“)
  gat_heads: 2
  gat_dropout: 0.12541721858747987
  gat_v2: true
  gat_edge_dim: 64
  
  # MLP Configuration (from failed params - converted from dynamic list)
  use_node_mlp: true
  use_edge_mlp: true
  node_hidden_dims: [480, 32, 352, 256]  # 4 layers as specified
  edge_hidden_dims: [32, 416]  # 2 layers as specified
  
  # General Configuration (from failed params)
  activation: "elu"
  dropout_rate: 0.48206043808097504
  use_batch_norm: false
  use_residual: false
  use_skip_connections: true
  pooling: "mean"
  
  # Switch Head Configuration (MLP type - no attention heads needed)
  switch_head_type: "mlp"
  switch_head_layers: 4
  # Note: switch_attention_heads should be ignored for MLP type
  
  # Physics Configuration (from failed params)
  use_gated_mp: true
  use_phyr: false  # phyr_k_ratio should be ignored
  
  # Loss and normalization
  loss_scaling_strategy: "adaptive_magnitude"
  normalization_type: "per_node"

# Training Configuration (from failed params)
learning_rate: 0.005589299134044199
weight_decay: 1.1124827202322426e-05
batch_size: 32
epochs: 10  # Reduced for debugging
patience: 5

# Physics Loss Weights (from failed params)
lambda_phy_loss: 0.029875900764771497
lambda_connectivity: 0.005255539725273635
lambda_radiality: 0.13226810449268056
lambda_mask: 0.01

# Output configuration
output_type: "binary"  # Since using FocalLoss
num_classes: 2

# Data Loading Configuration
dataset_type: "default"
batching_type: "dynamic"
max_nodes: 1000
max_edges: 5000
train_ratio: 0.85
seed: 42
num_workers: 0  # For debugging

# Criterion (from failed params)
criterion_name: "FocalLoss"

# Evaluation
evaluate_every_x_epoch: 2
wandb: false  # Disable for debugging