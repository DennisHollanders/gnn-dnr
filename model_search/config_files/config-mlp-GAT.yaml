description: "GIN model with hyperparameter optimization"
job_name: "None"

# Data Configuration
dataset_names: ["train", "validation", "test"]
folder_names:
  - "data\\split_datasets\\train"
  - "data\\split_datasets\\validation"  
  - "data\\split_datasets\\test"
dataset_type: "default"
batching_type: "dynamic"
max_nodes: 20000
max_edges: 50000
batch_size: 128
train_ratio: 0.85
seed: 0
num_workers: 0

# Training Configuration
learning_rate: 0.0004977456423587269
weight_decay: 2.0808634647507578e-06
epochs: 300
patience: 10
criterion_name: "FocalLoss"

# Physics Loss Configuration 
lambda_connectivity: 0.06497262554235322
lambda_mask: 0.2333442569508463
lambda_phy_loss: 6.125445116538297
lambda_radiality: 1.124743240361359
loss_scaling_strategy: adaptive_ratio
normalization_type: adaptive


# Model Configuration
model_module: "AdvancedMLP"
model_kwargs:
  criterion_name: "FocalLoss"
  # Output Configuration
  output_type: "multiclass"
  num_classes: 2
  
  # GNN Configuration
  gnn_type: "GAT"
  gnn_layers: 2
  gnn_hidden_dim: 384 
  gat_heads: 4
  dropout_rate: 0.005998322292205552
  gat_dropout: 0.026125510066810693
  gin_eps: 0.0  

  # MLP Configuration
  node_hidden_dims: []
  use_node_mlp: false 
  edge_hidden_dims: [64, 512, 64]
  use_edge_mlp: true

  # General Configuration
  activation: "leaky_relu"
  dropout_rate: 0.0
  use_batch_norm: true
  use_residual: true
  use_skip_connections: true
  pooling: "add"

  # Switch Head Configuration
  switch_head_type: "mlp"
  switch_head_layers: 5
  switch_attention_heads: 0

  # Physics Configuration
  use_gated_mp: true
  use_phyr: false
  enforce_radiality: false

  # Physics Loss Configuration
  lambda_connectivity: 0.06497262554235322
  lambda_mask: 0.2333442569508463
  lambda_phy_loss: 6.125445116538297
  lambda_radiality: 1.124743240361359
  loss_scaling_strategy: adaptive_ratio
  normalization_type: adaptive

# Logging
wandb: true
wandb_project: "GIN_HPO"