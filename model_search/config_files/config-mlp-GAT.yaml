description: "GIN model with hyperparameter optimization"
job_name: "None"

# Data Configuration
dataset_names: ["train", "validation", "test"]
folder_names:
  - "data\\split_datasets\\train"
  - "data\\split_datasets\\validation"  
  - "data\\split_datasets\\test"
dataset_type: "default"
batching_type: "dynamic"
max_nodes: 20000
max_edges: 50000
batch_size: 128
train_ratio: 0.85
seed: 0
num_workers: 0

# Training Configuration
learning_rate: 0.00035760499745340354
weight_decay: 3.1083921460064515e-05
epochs: 200
patience: 20
criterion_name: "FocalLoss"

# Physics Loss Configuration (used in training loop)
lambda_phy_loss: 0.96543480711283145
lambda_mask: 0.01
lambda_connectivity: 0.0028830179868523345
lambda_radiality: 0.001834148675023315
loss_scaling_strategy: "fixed"
normalization_type: "adaptive"

# Hyperparameter Search
hp_search: false
hp_search_n_trials: 50

# Model Configuration
model_module: "AdvancedMLP"
model_kwargs:
  # Output Configuration
  output_type: "multiclass"
  num_classes: 2
  
  # GNN Configuration
  gnn_type: "GAT"
  gnn_layers: 2
  gnn_hidden_dim: 224  # Fixed from gnn_hidden_dim_multiple
  gat_heads: 2
  gat_dropout: 0.05099716381479796
  gin_eps: 0.0  # Added (for GIN layers)

  # MLP Configuration
  use_node_mlp: true
  node_hidden_dims: [256, 512, 512]
  use_edge_mlp: true
  edge_hidden_dims: [256, 512]

  # General Configuration
  activation: "leaky_relu"
  dropout_rate: 0.0
  use_batch_norm: true
  use_residual: true
  use_skip_connections: true
  pooling: "add"


  # Switch Head Configuration
  switch_head_type: "graph_attention"
  switch_head_layers: 4
  switch_attention_heads: 4  # Added

  # Physics Configuration
  use_gated_mp: true
  use_phyr: false
  enforce_radiality: false  # Added

# Logging
wandb: true
wandb_project: "GIN_HPO"