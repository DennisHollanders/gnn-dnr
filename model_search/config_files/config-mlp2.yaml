description: "Debug Config 2 - Graph Attention Switch Head Case"
job_name: "debug_graph_attention_switch_head"

# Data Configuration
dataset_names: ["train", "validation", "test"]
folder_names:
  - "data\\split_datasets\\train"
  - "data\\split_datasets\\validation"
  - "data\\split_datasets\\test"

# Model Configuration
model_module: "AdvancedMLP"  
model_kwargs:
  # GAT Configuration (from failed params)
  gnn_type: "GAT"  
  gnn_layers: 6
  gnn_hidden_dim: 192  # Should be divisible by gat_heads (192 % 4 = 0 âœ“)
  gat_heads: 4
  gat_dropout: 0.3157398797200728
  gat_v2: true
  gat_edge_dim: 16
  
  # MLP Configuration (from failed params - converted from dynamic list)
  use_node_mlp: true
  use_edge_mlp: true
  node_hidden_dims: [192, 32, 128, 64]  # 4 layers as specified
  edge_hidden_dims: [288, 128, 256]  # 3 layers as specified
  
  # General Configuration (from failed params)
  activation: "tanh"
  dropout_rate: 0.5154461036366417
  use_batch_norm: true
  use_residual: false
  use_skip_connections: false
  pooling: "mean"
  
  # Switch Head Configuration (ATTENTION type - this is where the error likely occurs)
  switch_head_type: "graph_attention"
  switch_head_layers: 2
  switch_attention_heads: 1  # PROBLEM: 192 % 1 = 0 (should work, but might cause shape issues)
  
  # Physics Configuration (from failed params)
  use_gated_mp: true
  use_phyr: true
  phyr_k_ratio: 0.46988246916295767
  
  # Loss and normalization
  loss_scaling_strategy: "adaptive_magnitude"
  normalization_type: "adaptive"

# Training Configuration (from failed params)
learning_rate: 0.00039501184838746327
weight_decay: 1.7382941938393353e-05
batch_size: 32
epochs: 10  # Reduced for debugging
patience: 5

# Physics Loss Weights (from failed params)
lambda_phy_loss: 0.8595382613595642
lambda_connectivity: 0.004081803712424774
lambda_radiality: 0.023794696723754897
lambda_mask: 0.01

# Output configuration
output_type: "regression"  # Since using MSELoss
num_classes: 2  # May not be used for regression

# Data Loading Configuration
dataset_type: "default"
batching_type: "dynamic"
max_nodes: 1000
max_edges: 5000
train_ratio: 0.85
seed: 42
num_workers: 0  # For debugging

# Criterion (from failed params)
criterion_name: "MSELoss"

# Evaluation
evaluate_every_x_epoch: 2
wandb: false  # Disable for debugging