# GCN HPO Configuration
# Estimated trials needed: 24 dimensions Ã— 10 = 240 trials

# Basic model configuration
model_module: "AdvancedMLP"
dataset_names: ["train",
                  "validation",
                  "test"]
folder_names:
  - "data\\split_datasets\\train"
  - "data\\split_datasets\\validation"
  - "data\\split_datasets\\test"
dataset_type: "default"
batching_type: "dynamic"

# Fixed parameters (not optimized)
fixed_params:
  # Core GNN settings
  gnn_type: "GCN"  
  use_node_mlp: true
  use_edge_mlp: true
  
  # Data settings
  batch_size: 32
  max_nodes: 1000
  max_edges: 5000
  train_ratio: 0.85
  num_workers: 0
  seed: 42
  
  # Training settings
  epochs: 200
  patience: 15
  
  # Output settings 
  output_type: "binary"  
  num_classes: 2  
  
  # W&B settings
  wandb_project: "GCN_HPO"
  
# Search space - 24 dimensions
search_space:
  # 1. Learning rate (log scale)
  learning_rate:
    search_type: "float"
    min: 0.00001
    max: 0.01
    log: true
    
  # 2. Weight decay (log scale)
  weight_decay:
    search_type: "float"
    min: 0.000001
    max: 0.001
    log: true
    
  # 3. Dropout rate
  dropout_rate:
    search_type: "float"
    min: 0.0
    max: 0.7
    
  # 4. GNN layers (specific to backbone)
  gnn_layers:
    search_type: "int"
    min: 1
    max: 6
    
  # 5. GNN hidden dimension
  gnn_hidden_dim:
    search_type: "int"
    min: 32
    max: 512
    step: 32
    
  # 6. Node MLP hidden dimensions (dynamic list)
  node_hidden_dims:
    search_type: "dynamic_list"
    n_layers_min: 1
    n_layers_max: 4
    dim_min: 32
    dim_max: 512
    dim_step: 32
    
  # 7. Edge MLP hidden dimensions (dynamic list)
  edge_hidden_dims:
    search_type: "dynamic_list"
    n_layers_min: 1
    n_layers_max: 4
    dim_min: 32
    dim_max: 512
    dim_step: 32
    
  # 8. Activation function
  activation:
    search_type: "categorical"
    choices: ["relu", "gelu", "tanh", "elu", "leaky_relu", "sigmoid"]
    
  # 9. Use batch normalization
  use_batch_norm:
    search_type: "categorical"
    choices: [true, false]
    
  # 10. Use residual connections
  use_residual:
    search_type: "categorical"
    choices: [true, false]
    
  # 11. Use skip connections
  use_skip_connections:
    search_type: "categorical"
    choices: [true, false]
    
  # 12. Switch head type
  switch_head_type:
    search_type: "categorical"
    choices: ["mlp", "attention","graph_attention"]
    
  # 13. Switch head layers
  switch_head_layers:
    search_type: "int"
    min: 1
    max: 4
    
  # 14. Criterion/Loss function
  criterion_name:
    search_type: "categorical"
    choices: ["MSELoss", "CrossEntropyLoss",  "WeightedBCELoss","FocalLoss" ]
    
  # 15. Normalization type
  normalization_type:
    search_type: "categorical"
    choices: ["none","simple", "injection_scale","adaptive", "per_node"]
    
  # 16. Loss scaling strategy
  loss_scaling_strategy:
    search_type: "categorical"
    choices: ["fixed", "adaptive_ratio", "adaptive_magnitude", "uncertainty_weighted"]
    
  # 17. Use gated message passing
  use_gated_mp:
    search_type: "categorical"
    choices: [true, false]
    
  # 18. Use PhyR (Physics-enhanced Reasoning)
  use_phyr:
    search_type: "categorical"
    choices: [true, false]
    
  # 19. PhyR k ratio (only if use_phyr=true)
  phyr_k_ratio:
    search_type: "float"
    min: 0.1
    max: 0.5
    
  # 20. Physics loss weight
  lambda_phy_loss:
    search_type: "float"
    min: 0.01
    max: 1.0
    log: true
    
  # 21. Connectivity loss weight
  lambda_connectivity:
    search_type: "float"
    min: 0.001
    max: 0.5
    log: true
    
  # 22. Radiality loss weight
  lambda_radiality:
    search_type: "float"
    min: 0.001
    max: 0.5
    log: true
    
  # 23. Mask/sparsity loss weight
  lambda_mask:
    search_type: "float"
    min: 0.001
    max: 0.1
    log: true
    
  # 24. Graph pooling method
  pooling:
    search_type: "categorical"
    choices: ["mean", "max", "add"]