description: "GAT model with hyperparameter optimization"
job_name: "GAT_HPO_Large"

# Data Configuration
dataset_names: ["train", "validation", "test"]
folder_names:
  - "data/split_datasets/train"
  - "data/split_datasets/validation"
  - "data/split_datasets/test"
dataset_type: "default"
batching_type: "dynamic"
max_nodes: 20000
max_edges: 50000
train_ratio: 0.85
seed: 0
num_workers: 4

# Model Configuration
model_module: "AdvancedMLP"
model_kwargs:
  # GAT Configuration
  gnn_type: GAT
  gnn_layers: 2
  gnn_hidden_dim: 256 
  gat_heads: 4
  gat_dropout: 0.005
  gat_v2: true

  # Edge MLP
  use_edge_mlp: true
  edge_hidden_dims: [256, 512]

  # Node MLP
  use_node_mlp: true
  node_hidden_dims: [256, 512, 512]
  # General Configuration
  activation: "leaky_relu"
  dropout_rate:  0.01
  use_batch_norm: true
  use_residual: true
  use_skip_connections: true
  pooling: "add"
  normalization_type: "per_node"

  # Switch Head
  switch_head_type: "mlp"
  switch_head_layers: 3

  # Physics losses
  use_gated_mp: true
  use_phyr: false
  lambda_connectivity: 1e-1
  lambda_phy_loss: 0.7
  lambda_radiality: 1e-2

  # Optimizer
  learning_rate: 0.00035760499745340354
  weight_decay: 3.1083921460064515e-05

  # Loss & Optimization
  loss_scaling_strategy: "adaptive_magnitude"
  criterion_name: "WeightedBCELoss"
criterion_name: "WeightedBCELoss"
# Training Configuration
batch_size: 128
epochs: 200
patience: 20
learning_rate: 0.00035760499745340354
weight_decay: 3.1083921460064515e-05
# Output Configuration
output_type: "mutliclass"
num_classes: 2

# Logging
wandb_project: "GAT_HPO_Large"
wandb: true
