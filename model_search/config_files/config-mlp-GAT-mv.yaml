# Test working config in HPO - based on your config-mlp-GAT.yaml
# This config has NO search space - it uses fixed parameters that work in main.py

model_module: "AdvancedMLP"
dataset_names: ["train", "validation", "test"]
folder_names: 
  - "data\\split_datasets\\train"
  - "data\\split_datasets\\validation"
  - "data\\split_datasets\\test"
dataset_type: "default"
batching_type: "dynamic"
batch_size: 128

# All parameters as fixed_params (no search)
fixed_params:
  # Data settings
  max_nodes: 20000
  max_edges: 50000
  train_ratio: 0.85
  seed: 0
  num_workers: 0
  
  # Model architecture - copied from your working config
  gnn_type: "GAT"
  gat_dropout: 0.05099716381479796
  gat_heads: 2
  gnn_layers: 4
  gnn_hidden_dim: 448  # 224 * 2 heads
  
  # MLP settings
  use_edge_mlp: true
  edge_hidden_dims: [128, 256]
  use_node_mlp: true  
  node_hidden_dims: [64, 64, 128, 256]
  
  # General settings
  activation: "leaky_relu"
  dropout_rate: 0.0013447489600537238
  use_batch_norm: true
  use_residual: true
  use_skip_connections: true
  pooling: "add"
  normalization_type: "per_node"
  
  # Switch head
  switch_head_type: "mlp" 
  switch_head_layers: 4
  
  # Physics - IMPORTANT: These were missing in HPO
  use_gated_mp: true
  use_phyr: false
  lambda_connectivity: 0.0028830179868523345
  lambda_phy_loss: 0.96543480711283145
  lambda_radiality: 0.001834148675023315
  lambda_mask: 0.01  # Default value
  
  # Training
  learning_rate: 0.0009708944531483163
  weight_decay: 0.00015996260658050464
  criterion_name: "WeightedBCELoss"
  epochs: 50  # Shorter for testing
  patience: 10
  
  # Output
  output_type: "multiclass"
  num_classes: 2
  
  # Loss scaling - IMPORTANT: This was missing
  loss_scaling_strategy: "adaptive_magnitude"
  
  # Logging
  wandb_project: "HPO_Config_Test"

# Empty search space - this makes it a single trial with fixed config
search_space: {}