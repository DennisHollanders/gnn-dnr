description: "Two-stage HPO for GIN: Architecture search then hyperparameter tuning"
job_name: "GIN-two-stage-HPO"

# Data Configuration
dataset_names: ["train", "validation", "test"]
folder_names:
  - "data\\split_datasets\\train"
  - "data\\split_datasets\\validation"
  - "data\\split_datasets\\test"
dataset_type: "default"
batching_type: "dynamic"
max_nodes: 20000
max_edges: 50000
batch_size: 128
train_ratio: 0.85
seed: 0
num_workers: 0

# Model Configuration (fixed for all trials)
model_module: "AdvancedMLP"

# Fixed Parameters (Stage 1: Architecture Search)
fixed_params:
  # Output Configuration
  output_type: "multiclass"
  num_classes: 2
  
  # GNN Type (fixed for GIN experiments)
  gnn_type: "GIN"
  
  # Training Configuration (fixed during architecture search)
  learning_rate: 0.001
  weight_decay: 0.0001
  epochs: 100
  patience: 15
  criterion_name: "FocalLoss"
  
  # Physics Loss Configuration (fixed during architecture search)
  lambda_phy_loss: 0.5
  lambda_mask: 0.01
  lambda_connectivity: 0.01
  lambda_radiality: 0.01
  loss_scaling_strategy: "fixed"
  normalization_type: "adaptive"
  
  # General Configuration (fixed)
  activation: "leaky_relu"
  dropout_rate: 0.1
  use_batch_norm: true
  use_residual: true
  use_skip_connections: true
  pooling: "add"
  
  # Physics Configuration (fixed)
  use_gated_mp: true
  use_phyr: false
  enforce_radiality: false
  
  # GIN-specific fixed params
  gin_eps: 0.1
  gin_train_eps: false
  gat_heads: 4
  gat_dropout: 0.1

# STAGE 1: ARCHITECTURE SEARCH SPACE
search_space:
  # GNN Architecture
  gnn_layers:
    search_type: "int"
    min: 1
    max: 4
  
  gnn_hidden_dim:
    search_type: "categorical"
    choices: [64, 128, 192, 256, 320, 384, 448, 512, 576, 640, 704, 768]
  
  # GIN-specific architecture
  gin_mlp_layers:
    search_type: "int"
    min: 1
    max: 4
  
  # Node MLP Architecture
  use_node_mlp:
    search_type: "categorical"
    choices: [true, false]
  
  node_hidden_dims:
    search_type: "dynamic_list"
    n_layers_min: 1
    n_layers_max: 4
    dim_min: 64
    dim_max: 1024
  
  # Edge MLP Architecture
  use_edge_mlp:
    search_type: "categorical"
    choices: [true, false]
  
  edge_hidden_dims:
    search_type: "dynamic_list"
    n_layers_min: 1
    n_layers_max: 4
    dim_min: 64
    dim_max: 1024
  
  # Switch Head Architecture
  switch_head_type:
    search_type: "categorical"
    choices: ["mlp", "attention", "gated_attention"]
  
  switch_head_layers:
    search_type: "int"
    min: 1
    max: 5
  
  switch_attention_heads:
    search_type: "categorical"
    choices: [1, 2, 4, 8]

# STAGE 2: HYPERPARAMETER TUNING (COMMENTED OUT)
# Uncomment and replace search_space above for Stage 2
# search_space:
#   # Training Hyperparameters
#   learning_rate:
#     search_type: "float"
#     min: 0.0001
#     max: 0.01
#     log: true
#   
#   weight_decay:
#     search_type: "float"
#     min: 1e-6
#     max: 1e-2
#     log: true
#   
#   dropout_rate:
#     search_type: "float"
#     min: 0.0
#     max: 0.5
#   
#   # GIN-specific hyperparameters
#   gin_eps:
#     search_type: "float"
#     min: 0.0
#     max: 0.5
#   
#   gin_train_eps:
#     search_type: "categorical"
#     choices: [true, false]
#   
#   # Physics Loss Hyperparameters
#   lambda_phy_loss:
#     search_type: "float"
#     min: 0.1
#     max: 2.0
#   
#   lambda_mask:
#     search_type: "float"
#     min: 0.001
#     max: 0.1
#   
#   lambda_connectivity:
#     search_type: "float"
#     min: 0.001
#     max: 0.1
#   
#   lambda_radiality:
#     search_type: "float"
#     min: 0.001
#     max: 0.1
#   
#   # Loss scaling strategy
#   loss_scaling_strategy:
#     search_type: "categorical"
#     choices: ["fixed", "adaptive", "adaptive_ratio"]
#   
#   normalization_type:
#     search_type: "categorical"
#     choices: ["adaptive", "standard", "robust"]
#   
#   # Batch size optimization
#   batch_size:
#     search_type: "categorical"
#     choices: [32, 64, 128, 256]

# Stage 2 Fixed Parameters (use best architecture from Stage 1)
# fixed_params:
#   # Best architecture from Stage 1 (example values - replace with actual best)
#   gnn_layers: 2
#   gnn_hidden_dim: 360
#   gin_mlp_layers: 3
#   use_node_mlp: true
#   node_hidden_dims: [256, 512, 256, 128]
#   use_edge_mlp: true
#   edge_hidden_dims: [128, 512, 64]
#   switch_head_type: "mlp"
#   switch_head_layers: 4
#   switch_attention_heads: 4
#   
#   # Fixed model configuration
#   output_type: "multiclass"
#   num_classes: 2
#   gnn_type: "GIN"
#   activation: "leaky_relu"
#   use_batch_norm: true
#   use_residual: true
#   use_skip_connections: true
#   pooling: "add"
#   use_gated_mp: true
#   use_phyr: false
#   enforce_radiality: true
#   gat_heads: 4
#   gat_dropout: 0.1
#   epochs: 200
#   patience: 20
#   criterion_name: "FocalLoss"

# Logging
wandb: true
wandb_project: "GIN_HPO_TwoStage"